{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192cef4b",
   "metadata": {},
   "source": [
    "# 1조 홍성진 Exploration 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30273fb4",
   "metadata": {},
   "source": [
    "# 1. 숫자 손글씨 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "afb3ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8a643",
   "metadata": {},
   "source": [
    "## 1-2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "3f1a9f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "print(dir(digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a058d8",
   "metadata": {},
   "source": [
    "## 1-3-1. Feature Data 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ac220b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_data = digits.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2302a",
   "metadata": {},
   "source": [
    "## 1-3-2. Label Data 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "125fc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_label = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7927dae",
   "metadata": {},
   "source": [
    "## 1-3-3. target Names 출력해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "36c419ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc8c95",
   "metadata": {},
   "source": [
    "## 1-3-4. 데이터 Describe 해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "c54ae384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e018cd0",
   "metadata": {},
   "source": [
    "## 1-4. train, test 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "fe742c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits_data,\n",
    "                                                    digits_label,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ca684",
   "metadata": {},
   "source": [
    "## 1-5. 다양한 모델로 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7547a",
   "metadata": {},
   "source": [
    "## 1-5-1. Decision Tree 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "b647fadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8694444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        31\n",
      "           1       0.80      0.84      0.82        38\n",
      "           2       0.90      0.95      0.92        38\n",
      "           3       0.77      0.85      0.81        27\n",
      "           4       0.94      0.83      0.88        41\n",
      "           5       0.85      0.94      0.89        35\n",
      "           6       0.82      0.87      0.85        38\n",
      "           7       0.89      0.91      0.90        34\n",
      "           8       0.84      0.77      0.81        35\n",
      "           9       0.90      0.81      0.85        43\n",
      "\n",
      "    accuracy                           0.87       360\n",
      "   macro avg       0.87      0.87      0.87       360\n",
      "weighted avg       0.87      0.87      0.87       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=25) #시드지정\n",
    "decision_tree.fit(X_train, y_train) # 모델 fit 시키기\n",
    "y_pred = decision_tree.predict(X_test) #예측\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred)) # 평가 지표를 이용한 모델평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed60f111",
   "metadata": {},
   "source": [
    "## 1-5-2. Random Forest 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "b5eb78bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9722222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95        31\n",
      "           1       0.95      0.97      0.96        38\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.96      0.96      0.96        27\n",
      "           4       0.95      0.98      0.96        41\n",
      "           5       0.97      1.00      0.99        35\n",
      "           6       1.00      0.95      0.97        38\n",
      "           7       0.97      1.00      0.99        34\n",
      "           8       1.00      0.94      0.97        35\n",
      "           9       0.95      0.98      0.97        43\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92615e9",
   "metadata": {},
   "source": [
    "## 1-5-3. SVM 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "294cac9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9833333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        31\n",
      "           1       0.95      1.00      0.97        38\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.96      0.96      0.96        27\n",
      "           4       0.98      0.98      0.98        41\n",
      "           5       1.00      1.00      1.00        35\n",
      "           6       1.00      1.00      1.00        38\n",
      "           7       1.00      1.00      1.00        34\n",
      "           8       0.97      0.94      0.96        35\n",
      "           9       0.98      0.98      0.98        43\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdbfd3c",
   "metadata": {},
   "source": [
    "## 1-5-4. SGD Classifier 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b0fa2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9444444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        31\n",
      "           1       0.97      0.84      0.90        38\n",
      "           2       1.00      0.97      0.99        38\n",
      "           3       1.00      0.89      0.94        27\n",
      "           4       0.98      0.98      0.98        41\n",
      "           5       1.00      0.89      0.94        35\n",
      "           6       0.97      1.00      0.99        38\n",
      "           7       0.94      1.00      0.97        34\n",
      "           8       0.77      0.97      0.86        35\n",
      "           9       0.89      0.93      0.91        43\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.95      0.94      0.95       360\n",
      "weighted avg       0.95      0.94      0.95       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_model = SGDClassifier()\n",
    "\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb7ff6",
   "metadata": {},
   "source": [
    "## 1-5-5. Logistic Regression 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "357b7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9694444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        31\n",
      "           1       0.95      0.97      0.96        38\n",
      "           2       1.00      0.97      0.99        38\n",
      "           3       0.96      0.93      0.94        27\n",
      "           4       0.93      1.00      0.96        41\n",
      "           5       0.94      0.97      0.96        35\n",
      "           6       1.00      0.97      0.99        38\n",
      "           7       1.00      1.00      1.00        34\n",
      "           8       0.94      0.94      0.94        35\n",
      "           9       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression(max_iter = 2000)\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad54b7",
   "metadata": {},
   "source": [
    "## 모델을 평가해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3073fd02",
   "metadata": {},
   "source": [
    "우선, 숫자 손글씨의 경우에는, 실습할 때에 0이라고하면 accuracy가 올라가는것 처럼,\n",
    "맞는것을 틀렸다고 하는 것 보다, 틀린것을 맞았다고 하는것이 치명적일 수 있다.\n",
    "즉, TP를 FN로 판단하면 안되기 때문에, 평가지표 중 Recall(재현율)이 중요하다 생각된다.\n",
    "그리고 현재 나의 모델 예측상, SVM모델이 손글씨 분류에 가장 좋은 결과를 보여준것으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fcd5b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d0868",
   "metadata": {},
   "source": [
    "# 2-1. 와인분류기 모듈 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "80c34c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc46a5",
   "metadata": {},
   "source": [
    "## 2-2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e1e549bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "wine = load_wine()\n",
    "print(dir(wine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29f590",
   "metadata": {},
   "source": [
    "## 2-3. 데이터 지정 및 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "b7a0f4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class_0', 'class_1', 'class_2'], dtype='<U7')"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data = wine.data\n",
    "wine_label = wine.target\n",
    "\n",
    "wine.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "3cf132c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695d2f5",
   "metadata": {},
   "source": [
    "## 2-4. 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "59d668e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine_data,\n",
    "                                                    wine_label,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ab4e0",
   "metadata": {},
   "source": [
    "## 2-5. 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050ee0d",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "3304da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8611111111111112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84         9\n",
      "           1       0.83      0.88      0.86        17\n",
      "           2       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.88      0.86      0.86        36\n",
      "weighted avg       0.87      0.86      0.86        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94151b",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "f86a1480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19651853",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "b0a05acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       0.74      0.82      0.78        17\n",
      "           2       0.56      0.50      0.53        10\n",
      "\n",
      "    accuracy                           0.75        36\n",
      "   macro avg       0.76      0.74      0.75        36\n",
      "weighted avg       0.75      0.75      0.75        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 0 값이 있어서 관련 오류가 나므로 삽입하였다.\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b56704",
   "metadata": {},
   "source": [
    "## SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "0e5c5820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6388888888888888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.89      0.76         9\n",
      "           1       0.62      0.88      0.73        17\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.64        36\n",
      "   macro avg       0.43      0.59      0.50        36\n",
      "weighted avg       0.46      0.64      0.54        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_model = SGDClassifier()\n",
    "\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12789c7",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "5c02ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9722222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       0.94      1.00      0.97        17\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.96      0.97        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49411cd",
   "metadata": {},
   "source": [
    "## 2-6. 모델을 평가해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e859f",
   "metadata": {},
   "source": [
    "위의 와인분류기 같은 경우, 우선, SVM, SGD Classifier 모델에 경우, 학습률이 매우 낮았다. \n",
    "와인분류기에는 적절하지 못한 모델이었던것 같다.\n",
    "모델 중에 Random Forest는 무려 1이라는 뛰어난 학습력을 가졌다. 평가지표에도 검출 되지 않은걸 보아, 오차는 없는것으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e741c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8551d62",
   "metadata": {},
   "source": [
    "# 3-1. 유방암 여부 모듈 가져오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "44e41faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72662dd6",
   "metadata": {},
   "source": [
    "## 3-2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "1f31826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'feature_names', 'filename', 'frame', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print(dir(cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97dab0",
   "metadata": {},
   "source": [
    "## 3-3. 데이터 지정 및 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "316e31c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data = cancer.data\n",
    "cancer_label = cancer.target\n",
    "\n",
    "cancer.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "94741843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c179cf1",
   "metadata": {},
   "source": [
    "## 3-4. 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "98c61bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer_data,\n",
    "                                                    cancer_label,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449467c5",
   "metadata": {},
   "source": [
    "## 2-5. 데이터 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb4007",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "8e1de8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9385964912280702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93        49\n",
      "           1       0.94      0.95      0.95        65\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29997d2",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "e6a7d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94        49\n",
      "           1       0.94      0.97      0.95        65\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.94      0.95       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8569e8",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "e8efbcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94        49\n",
      "           1       0.93      0.98      0.96        65\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.94      0.95       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.SVC()\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c78ec4",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "14d8f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9122807017543859\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90        49\n",
      "           1       0.95      0.89      0.92        65\n",
      "\n",
      "    accuracy                           0.91       114\n",
      "   macro avg       0.91      0.92      0.91       114\n",
      "weighted avg       0.92      0.91      0.91       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_model = SGDClassifier()\n",
    "\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424dbaf3",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "456d03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.956140350877193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        49\n",
      "           1       0.95      0.97      0.96        65\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.96       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e108d",
   "metadata": {},
   "source": [
    "## 3-6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447e745",
   "metadata": {},
   "source": [
    "암 판단 모델 같은경우에는, TP를 FN로 판단하면 안되는 경우이므로, 평가지표 중 recall이 매우 중요하다.\n",
    "모델의 경우 SGD Classifier 모델이 결과 값이 비교적 낮게 나왔는데, recall의 라벨 1 부분 값도 낮게 나온것을 확인 할 수 있다.\n",
    "암 판단 모델 역시도, Random Forest의 모델이 비교적 가장 결과가 좋게 나온것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638a401",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33237ca",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916fe5f9",
   "metadata": {},
   "source": [
    "이번 Expolration 같은경우에는, 우선 이론적인 부분이 많아 애를 많이 먹었다.\n",
    "그렇다고 코딩이 쉽지는 않았는데, 어느정도는 LMS를 통해 구현이 잘 되어있어서 따라하였지만, 하나 하나 내것으로 만들려고 하다보니 시간이 다소 오래걸렸다.\n",
    "특히 데이터마다 각각의 오류가 나오는 부분이 다소 있어서 직접 해결 방법을 찾아내어 공부하였다. 이를테면, 손글씨 데이터의 경우에는, Logistic Regression모델을 돌릴 때에는 max_iter 이슈가 나와서, 값을 추가하여 오류를 제거 하였다. 그 때에도 무작정 높히지않고, 가장 최적의 값을 찾아 내었다.\n",
    "Wine데이터의 경우에는 SVM모델에서 워닝 오류가 나서, 검색에 명확한 해결 방안은 나오지 않아서, 그나마 가장 적합하다고 생각이 드는 원인을 찾았는데, 데이터 안에 0의 값이 있으므로 devision 오류가 났던것 같다. 때문에 임시로 warning을 'ignor' 해 주었는데, 이 방법이 임시방편인건지, 만약 그렇다면 결과 정확도 데이터에 영향을 다소 준건 아닌지 하는 걱정이 들었다. 이부분이 아직 해소가 되지 않은 부분이다.\n",
    "해소가 되지 않은 한 부분이 더 있었는데, wine 데이터와, 유방암 데이터의 경우에 왜 SVM모델과 SGD Classifier가 학습이 잘 되지 않았던건지, 이론적으로는 보았지만 아직 명확한 답을 해결하지 못한것 같다.\n",
    "\n",
    "루브릭 평가 지표를 맞추기 위해 위에 말했듯, 모든 오류들을 찾아 해결하려 나섰고, 또한 데이터 분리과정에서 seed값을 조정해가며 나름 최적의 환경을 만들어주려고 연구해 보았다. 또한, 모델 평가가 중요하다 생각하여, 왜 이렇게 나왔는가에 관해 조사도 많이하고 나름대로의 설득력 있는 답을 하기 위해 조사하여 답하였다.\n",
    "\n",
    "만약, 루브릭 평가 지표를 달성하지 못하였다면, 그 이유는 아마 모델평가에 대해 명확하지 않아서 이기 떄문이지 않을까 싶다. 나름대로 조사해서 이해한다고 하였으나, 아직 누군가에게 명확하게 답할 수 있을만큼은 아니라고 생각하기 때문이다. 아직 각각의 환경마다 어떤 모델을 써야하고, 어떤 평가 지표를 활용해야 하는지에 자신있게 설명할 수가 없기 때문이지 않을까 추정해본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
